{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.25.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium webdriver-manager pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing grant URLs\n",
    "input_csv_path = '/Users/miguelemb/Downloads/Projects/Grant scrapper/data/seed_url/Business Incubator Grants (1).csv'\n",
    "grants_df = pd.read_csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and extract URLs from the 'OPPORTUNITY NUMBER' column\n",
    "def clean_url(cell):\n",
    "    match = re.search(r'\\\"(http[^\"]+)\\\"', cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "grants_df['URL'] = grants_df['OPPORTUNITY NUMBER'].apply(clean_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing URLs\n",
    "grants_df = grants_df.dropna(subset=['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 510/510 [10:28<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract all visible text from a webpage\n",
    "def extract_all_text(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1)  # Allow time for the page to load\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract all text from the body of the page\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            text = body.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            text = \"\"\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Loop through each grant URL and extract data\n",
    "start_time = time.time()\n",
    "grants_data = []\n",
    "for idx, row in enumerate(tqdm(grants_df.iterrows(), total=len(grants_df), desc=\"Processing URLs\")):\n",
    "    url = row[1]['URL']\n",
    "    \n",
    "    # Extract all available text from the grant page\n",
    "    page_text = extract_all_text(url)\n",
    "    \n",
    "    # Store the extracted data, retaining original columns\n",
    "    grants_data.append({\n",
    "        'OPPORTUNITY NUMBER': row[1].get('OPPORTUNITY NUMBER', None),\n",
    "        'OPPORTUNITY TITLE': row[1].get('OPPORTUNITY TITLE', None),\n",
    "        'AGENCY CODE': row[1].get('AGENCY CODE', None),\n",
    "        'OPPORTUNITY STATUS': row[1].get('OPPORTUNITY STATUS', None),\n",
    "        'POSTED DATE': row[1].get('POSTED DATE', None),\n",
    "        'CLOSE DATE': row[1].get('CLOSE DATE', None),\n",
    "        'URL': url,\n",
    "        'Extracted Text': page_text\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 628.58 seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert the scraped data into a DataFrame\n",
    "scraped_grants_df = pd.DataFrame(grants_data)\n",
    "# Save the extracted data to a new CSV file\n",
    "output_csv_path = '/Users/miguelemb/Downloads/Projects/Grant scrapper/data/scraped_data/scraped_all_grants_2.csv'\n",
    "scraped_grants_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print the total time taken\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
